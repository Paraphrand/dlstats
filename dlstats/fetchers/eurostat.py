# -*- coding: utf-8 -*-
"""
.. module:: eurostat
    :platform: Unix, Windows
    :synopsis: Populate a MongoDB database with data from Eurostat

.. :moduleauthor :: Widukind team <widukind-dev@cepremap.org>
"""

from datetime import datetime
import logging
import os
import zipfile

from lxml import etree

from widukind_common import errors

from dlstats import constants
from dlstats.utils import Downloader, clean_datetime
from dlstats.fetchers._commons import Fetcher, Datasets, Providers, SeriesIterator
from dlstats.xml_utils import (XMLStructure_2_0 as XMLStructure,
                               XMLCompactData_2_0_EUROSTAT as XMLData,
                               dataset_converter)


TABLE_OF_CONTENT_NSMAP = {'nt': 'urn:eu.europa.ec.eurostat.navtree',
                          'xsi': 'http://www.w3.org/2001/XMLSchema-instance'}

xpath_title = etree.XPath("nt:title[@language='en']/text()", namespaces=TABLE_OF_CONTENT_NSMAP)
xpath_code = etree.XPath("nt:code/text()", namespaces=TABLE_OF_CONTENT_NSMAP)
xpath_ds_last_update = etree.XPath("nt:lastUpdate/text()", namespaces=TABLE_OF_CONTENT_NSMAP)
xpath_ds_last_modified = etree.XPath("nt:lastModified/text()", namespaces=TABLE_OF_CONTENT_NSMAP)
xpath_ds_metadata_html = etree.XPath("nt:metadata[@format='html']/text()", namespaces=TABLE_OF_CONTENT_NSMAP)

xpath_ds_data_start = etree.XPath("nt:dataStart/text()", namespaces=TABLE_OF_CONTENT_NSMAP)
xpath_ds_data_end = etree.XPath("nt:dataEnd/text()", namespaces=TABLE_OF_CONTENT_NSMAP)
xpath_ds_values = etree.XPath("nt:values/text()", namespaces=TABLE_OF_CONTENT_NSMAP)

VERSION = 4

logger = logging.getLogger(__name__)

def fixtag_toc(ns, tag, nsmap=TABLE_OF_CONTENT_NSMAP):
    return '{' + nsmap[ns] + '}' + tag

def first_element_xpath(values, default=None):
    """Return first element of array or default value
    """
    if values and isinstance(values, list) and len(values) > 0:
        return values[0]

    return default

def extract_zip_file(zipfilepath):
    """Extract first file in zip file and return absolute path for the file extracted

    :param str filepath: Absolute file path of zip file

    Example:
        file1.zip contains one file: file1.csv

    >>> extract_zip_file('/tmp/file1.zip')
    '/tmp/file1.csv'

    """
    zfile = zipfile.ZipFile(zipfilepath)
    filepaths = {}
    for filename in zfile.namelist():
        filepaths.update({filename: zfile.extract(filename,
                                                  os.path.dirname(zipfilepath))})
    return filepaths

def make_url(dataset_code):
    return("http://ec.europa.eu/eurostat/" +
           "estat-navtree-portlet-prod/" +
           "BulkDownloadListing?sort=1&file=data/" +
           dataset_code + ".sdmx.zip")

class Eurostat(Fetcher):
    """Class for managing the SDMX endpoint from eurostat in dlstats."""

    def __init__(self, **kwargs):
        super().__init__(provider_name='EUROSTAT', version=VERSION, **kwargs)

        self.provider = Providers(name=self.provider_name,
                                  long_name='Eurostat',
                                  version=VERSION,
                                  region='Europe',
                                  website='http://ec.europa.eu/eurostat',
                                  terms_of_use='http://ec.europa.eu/eurostat/about/our-partners/copyright',
                                  fetcher=self)

        self.categories_filter = [
            'nama10',
            'namq_10',
            'nasa_10',
            'nasq_10',
            'naid_10',
            'nama',
            'namq',
            'nasa',
            'nasq',
            'gov',
            'ert',
            'irt',
            'prc',
            'bop',
            'bop_6',
            'demo',  # We harvest demo because we need demo_pjanbroad.
            'lfsi_act_q',
            'euroind',
            'pop',
            'labour',
        ]

        self.url_table_of_contents = "http://ec.europa.eu/eurostat/estat-navtree-portlet-prod/BulkDownloadListing?sort=1&file=table_of_contents.xml"
        self.updated_catalog = False

    def _is_updated_catalog(self, creation_date):

        if not self.provider.from_db:
            self.provider_verify()

        if not self.provider.metadata:
            self.provider.metadata = {}

        if not "creation_date" in self.provider.metadata:
            self.provider.metadata["creation_date"] = creation_date
            self.provider.update_database()
            return True

        if creation_date > self.provider.metadata["creation_date"]:
            self.provider.metadata["creation_date"] = creation_date
            self.provider.update_database()
            return True

        return False

    def build_data_tree(self):
        """Builds the data tree
        """

        download = Downloader(url=self.url_table_of_contents,
                              filename="table_of_contents.xml",
                              store_filepath=self.store_path,
                              use_existing_file=self.use_existing_file)
        filepath = download.get_filepath()

        categories = []
        categories_keys = []

        it = etree.iterparse(filepath, events=['end'], tag="{urn:eu.europa.ec.eurostat.navtree}leaf")

        def is_selected(parent_codes):
            """parent_codes is array of category_code
            """
            for _select in self.categories_filter:
                if _select in parent_codes:
                    return True
            return False

        def get_category(category_code):
            for c in categories:
                if c["category_code"] == category_code:
                    return c

        def create_categories(parent_codes, parent_titles, position):

            position += 1

            for i in range(len(parent_codes)):
                category_code = parent_codes.pop()
                name = parent_titles.pop()
                all_parents = parent_codes.copy()
                parent = None
                if all_parents:
                    parent = all_parents[-1]
                if not category_code in categories_keys:
                    _category = {
                        "provider_name": self.provider_name,
                        "category_code": category_code,
                        "name": name,
                        "position": position + i,
                        "parent": parent,
                        'all_parents': all_parents,
                        "datasets": [],
                        "doc_href": None,
                        "metadata": None
                    }
                    categories_keys.append(category_code)
                    categories.append(_category)

        position = 0
        is_verify_creation_date = False

        for event, dataset in it:

            if is_verify_creation_date is False:
                _root = dataset.getroottree().getroot()
                creation_date_str = _root.attrib.get("creationDate")
                creation_date = clean_datetime(datetime.strptime(creation_date_str,
                                                                 '%Y%m%dT%H%M'))

                if self._is_updated_catalog(creation_date) is False:
                    msg = "no update from eurostat catalog. current[%s] - db[%s]"
                    logger.warning(msg % (creation_date, self.provider.metadata["creation_date"]))
                    if not self.force_update:
                        return []

                is_verify_creation_date = True
                if not self.force_update:
                    self.updated_catalog = True

            parent_codes = dataset.xpath("ancestor::nt:branch/nt:code/text()", namespaces=TABLE_OF_CONTENT_NSMAP)

            if not is_selected(parent_codes):
                continue

            parent_titles = dataset.xpath("ancestor::nt:branch/nt:title[attribute::language='en']/text()", namespaces=TABLE_OF_CONTENT_NSMAP)
            category_code = parent_codes[-1]

            create_categories(parent_codes, parent_titles, position)

            category = get_category(category_code)

            name = xpath_title(dataset)[0]
            last_update = xpath_ds_last_update(dataset)
            last_modified = xpath_ds_last_modified(dataset)
            doc_href = xpath_ds_metadata_html(dataset)
            data_start = xpath_ds_data_start(dataset)
            data_end = xpath_ds_data_end(dataset)
            values = xpath_ds_values(dataset)

            last_update = datetime.strptime(last_update[0], '%d.%m.%Y')
            if last_modified:
                last_modified = datetime.strptime(last_modified[0], '%d.%m.%Y')
                last_update = max(last_update, last_modified)

            dataset_code = xpath_code(dataset)[0]
            _dataset = {
                "dataset_code": dataset_code,
                "name": name,
                "last_update": clean_datetime(last_update),
                "metadata": {
                    "doc_href": first_element_xpath(doc_href),
                    "data_start": first_element_xpath(data_start),
                    "data_end": first_element_xpath(data_end),
                    "values": int(first_element_xpath(values, default="0")),
                }
            }
            category["datasets"].append(_dataset)

        self.for_delete.append(filepath)

        return categories

    def upsert_dataset(self, dataset_code):
        """Updates data in Database for selected datasets
        """
        self.get_selected_datasets()

        doc = self.db[constants.COL_DATASETS].find_one(
            {'provider_name': self.provider_name, 'dataset_code': dataset_code},
            {'dataset_code': 1, 'last_update': 1})

        dataset_settings = self.selected_datasets[dataset_code]

        if doc and  doc['last_update'] >= dataset_settings['last_update']:
            comments = "update-date[%s]" % doc['last_update']
            raise errors.RejectUpdatedDataset(provider_name=self.provider_name,
                                              dataset_code=dataset_code,
                                              comments=comments)

        dataset = Datasets(provider_name=self.provider_name,
                           dataset_code=dataset_code,
                           name=dataset_settings["name"],
                           doc_href=dataset_settings["metadata"].get("doc_href"),
                           last_update=None,
                           fetcher=self)
        dataset.last_update = dataset_settings["last_update"]

        dataset.series.data_iterator = EurostatData(dataset)

        return dataset.update_database()

    def get_calendar(self):

        yield {
            "action": "update-fetcher",
            "period_type": "cron",
            "kwargs": {
                "provider_name": self.provider_name
            },
            "period_kwargs": {
                "day": '*',
                "hour": 11,
                "minute": 1,
                "timezone": 'Europe/Paris'
            }
        }

        yield {
            "action": "update-fetcher",
            "period_type": "cron",
            "kwargs": {
                "provider_name": self.provider_name
            },
            "period_kwargs": {
                "day": '*',
                "hour": 23,
                "minute": 1,
                "timezone": 'Europe/Paris'
            }
        }

    def load_datasets_update(self):

        datasets_list = self.datasets_list()
        if not self.updated_catalog and not self.force_update:
            msg = "update aborted for updated catalog"
            logger.warning(msg)

        dataset_codes = [d["dataset_code"] for d in datasets_list]

        #TODO: enable ?
        cursor = self.db[constants.COL_DATASETS].find(
            {'provider_name': self.provider_name,
             'dataset_code': {'$in': dataset_codes}},
            {'dataset_code': 1, 'last_update': 1})

        selected_datasets = {s['dataset_code'] : s for s in cursor}

        for dataset in datasets_list:
            dataset_code = dataset["dataset_code"]

            last_update_from_catalog = dataset['last_update']
            last_update_from_dataset = selected_datasets.get(dataset_code, {}).get('last_update')

            if (dataset_code not in selected_datasets) or (last_update_from_catalog > last_update_from_dataset):
                try:
                    self.wrap_upsert_dataset(dataset_code)
                except Exception as err:
                    if isinstance(err, errors.MaxErrors):
                        raise
                    msg = "error for provider[%s] - dataset[%s]: %s"
                    logger.critical(msg % (self.provider_name,
                                           dataset_code,
                                           str(err)))
            else:
                msg = "bypass update - provider[%s] - dataset[%s] - last-update-dataset[%s] - last-update-catalog[%s]"
                logger.info(msg % (self.provider_name, dataset_code, last_update_from_dataset, last_update_from_catalog))


class EurostatData(SeriesIterator):

    def __init__(self, dataset):
        super().__init__(dataset)

        self.dataset_url = make_url(self.dataset_code)

        self.xml_dsd = XMLStructure(provider_name=self.provider_name)

        self.store_path = self.get_store_path()

        self._load()

    def _load(self):

        download = Downloader(url=self.dataset_url,
                              filename="data-%s.zip" % self.dataset_code,
                              store_filepath=self.store_path,
                              use_existing_file=self.fetcher.use_existing_file)

        filepaths = (extract_zip_file(download.get_filepath()))
        dsd_fp = filepaths[self.dataset_code + ".dsd.xml"]
        data_fp = filepaths[self.dataset_code + ".sdmx.xml"]

        self.fetcher.for_delete.append(dsd_fp)
        self.fetcher.for_delete.append(data_fp)

        self.xml_dsd.process(dsd_fp)
        self._set_dataset()

        self.xml_data = XMLData(provider_name=self.provider_name,
                                dataset_code=self.dataset_code,
                                xml_dsd=self.xml_dsd,
                                dsd_id=self.dataset_code,
                                #TODO: frequencies_supported=FREQUENCIES_SUPPORTED
                                )
        self.rows = self.xml_data.process(data_fp)

    def _set_dataset(self):

        dataset = dataset_converter(self.xml_dsd, self.dataset_code)
        self.dataset.dimension_keys = dataset["dimension_keys"]
        self.dataset.attribute_keys = dataset["attribute_keys"]
        self.dataset.concepts = dataset["concepts"]
        self.dataset.codelists = dataset["codelists"]

    def build_series(self, bson):
        self.dataset.add_frequency(bson["frequency"])
        bson["last_update"] = self.dataset.last_update
        return bson

